1. Vision & Mission
To empower visually impaired individuals through a charitable, free-to-use intelligent navigation assistant. Leveraging offline multimodal AI as "intelligent eyes," our system enhances safety and autonomy by delivering real-time descriptive audio feedback about environmental obstacles, enabling safe navigation. Born at Google - The Gemma 3n Impact Challenge, we are committed to its long-term development as a non-profit resource.

2. Problem Statement
Navigating public and private spaces presents challenges for people with visual impairments. Unexpected obstacles like benches, signs, discarded items, and temporary barriers can pose significant safety risks. While traditional aids like canes and guide dogs are invaluable, they may not always provide information about obstacles at head height, their specific nature, or the safest path around them.

3. Solution Overview
This project combines a live video feed from a smartphone camera with a two-tiered AI architecture to deliver real-time guidance.
Constant Scanning:The system uses Google's lightweight MediaPipe framework to continuously scan the video feed for any potential objects.
Intelligent Analysis: When MediaPipe detects a significant object in the user's path, it triggers the more powerful Gemma 3n multimodal model.
Actionable Advice: Gemma analyzes the specific frame or video clip, understands the context of the scene, and generates natural language description of the obstacle and suggests a safe course of action (e.g., "There is a bench directly in your path; it is safest to step to your right.").
Audio Feedback: This text-based output is converted to speech, providing an audio alert to the user.

4. Key Features
Real-time Obstacle Detection: Uses the camera to identify potential hazards in real-time.
Intelligent Scene Analysis: Goes beyond simple detection ("box") to provide contextual understanding ("a box is blocking the pavement").
Natural Language Audio Alerts: Delivers clear, easy-to-understand voice commands and descriptions.
Efficient Two-Tiered Architecture: Optimizes performance and battery life by using lightweight MediaPipe for constant monitoring and only engaging the powerful Gemma model when necessary.
Accessible Technology: Built using common hardware (a smartphone) with on-device AI (Gemma 3B) and seamless integration of native accessibility features including TalkBack audio, haptic feedback, and high-contrast mode—to ensure universal usability.

5. Technical Architecture
The system's workflow is designed for efficiency and responsiveness:
Video Capture: A continuous video stream is captured from the device's camera using CameraX.

Tier 1 Analysis (MediaPipe - The "Scanner"):
Every frame is processed by MediaPipe's on-device Object Detection task.
This is extremely fast and has low computational overhead.
The system checks if any detected objects fall within a pre-defined " we are using models to measure object

Tier 2 Analysis (Gemma - The "Brain"):
An "event" is triggered only when a significant object enters the critical zone.
The relevant image frame (or a short video clip) is sent to the Gemma model.
The prompt sent with the image asks Gemma to act as an assistant for the visually impaired, focusing its analysis on identifying hazards and providing clear directions.
Audio Output (Text-to-Speech):
The descriptive text generated by Gemma is passed to a Text-to-Speech (TTS) engine.
The resulting audio is played through the device's speaker or headphones.
Cooldown Logic: To prevent overwhelming the user and the API, a cooldown period is enforced, ensuring that new alerts are only generated every few seconds.

6.Limitations and Future Work
Due to budget constraints, we were unable to run the Gemma 3n directly on-device, as the Android Studio emulator does not support the hardware acceleration required to run such models. While devices like the Pixel 8 or 9 are capable of supporting on-device inference with Gemma 3n, we currently do not have access to them and are not in a position to purchase them in the short term. We plan to acquire compatible hardware in the future to enable full on-device testing and deployment.

As a workaround, we moved our model testing pipeline to Google Colab. This enabled us to run various computer vision models for image identification and depth assessment in the cloud. We also used Google Colab to test Gemma 3n’s image and video-based inference capabilities, as well as to convert its text-based responses into audio output.

Looking ahead, our goal is to transition this pipeline to run natively on Android devices, allowing for real-time inference and improved user experience. We are currently fine-tuning Gemma 3n using the Ego4D dataset to improve its ability to detect and interpret obstacles from egocentric (first-person) video frames.
